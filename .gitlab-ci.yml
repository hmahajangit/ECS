default:
  image: cr.siemens.com/blrise/platform/infrastructure/dockerhub/ise-terraform-awscli:15.0.0

variables: 
  http_proxy: "$CODE_PROXY"
  https_proxy: "$CODE_PROXY"
  no_proxy: "127.0.0.1,localhost,cr.siemens.com,code.siemens.com,.siemens.com"
  PROJECT_NAME: nextwork
  NPM_CONFIG_REGISTRY: https://registry.npmjs.org/

stages:
  - Test and Lint
  - generate-cert
  - Build
  - Docker Image Push
  - hmnextwork Plan
  - hmnextwork Apply
  #- UAT Plan
  #- UAT Apply
  #- Production Plan
  #- Production Apply
  - Destroy

.set_aws_account_cred_script : &set_aws_account_cred_script
  - export AWS_REGION=$(region_var_name=AWS_REGION ; echo ${!region_var_name:-$DEFAULT_AWS_REGION})
  - |
    export $(printf "AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s" \
      $(aws sts assume-role-with-web-identity \
          --duration-seconds 3600 \
          --role-session-name "cicd" \
          --role-arn arn:aws:iam::$NEW_AWS_ACCOUNT_ID:role/$AWS_ROLE \
          --web-identity-token "${CI_JOB_JWT_V2}" \
          --query "Credentials.[AccessKeyId,SecretAccessKey,SessionToken]" \
          --output text 
      )
    )
  - echo "ECR_IMAGE_PATH=$NEW_AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/nextwork-repo" >> build.env
  - echo "AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" >> build.env
  - echo "AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" >> build.env
  - echo "AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN" >> build.env
  - export account_id="$(aws sts get-caller-identity --output text --query Account --region ${AWS_REGION})"
  - echo "AWS_ACCOUNT_ID=$account_id" >> build.env


.TF_VAR_backendconf_script: &TF_VAR_backendconf_script |
  export TF_VAR_PROJECT_NAME=$PROJECT_NAME && export TF_VAR_CI_PIPELINE_ID=$CI_PIPELINE_ID
  export TF_VAR_app_name=$CI_PROJECT_NAME
  export TF_VAR_aws_region=$AWS_REGION && export TF_VAR_workspace_key_prefix=$PROJECT_NAME
  export TF_VAR_remote_state_bucket=$PROJECT_NAME-$account_id-tfstate && export TF_VAR_remote_state_core_path=core/$PROJECT_NAME-core.tfstate
  export TF_VAR_remote_state_bucket_key=services/$CI_PROJECT_NAME-$PROJECT_NAME.tfstate
  export TF_VAR_dynamodb_tf_statelock=se-dynamodb-tf-statelock
  echo "using terraform backend from s3://$TF_VAR_remote_state_bucket/$TF_VAR_remote_state_bucket_key"
  echo "bucket  = \"$TF_VAR_remote_state_bucket\""      >  s3_backend.config
  echo "key     = \"$TF_VAR_remote_state_bucket_key\""  >> s3_backend.config
  echo "workspace_key_prefix  = \"$TF_VAR_workspace_key_prefix\"" >> s3_backend.config
  echo "region  = \"$TF_VAR_aws_region\""               >> s3_backend.config
  echo "dynamodb_table  = \"$TF_VAR_dynamodb_tf_statelock\""   >> s3_backend.config
  env | grep ^TF | sort || true

Test and Lint:
  image: cr.siemens.com/blrise/platform/infrastructure/dockerhub/ise-node-angular:16.10.0-14.2.10-alpine-openjdk11
  stage: Test and Lint
  script:
    - export NODE_TLS_REJECT_UNAUTHORIZED=0
    #  - npm i -D karma-chrome-launcher puppeteer sonar-scanner
    - npm install
    - ng test --configuration production --code-coverage=true
    - npm run sonar -X
  rules:
  - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME =~ /^(hmnextwork)$/ || $CI_COMMIT_BRANCH =~ /^(hmnextwork)$/'
  #- if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME =~ /^(production)$/ || $CI_COMMIT_BRANCH =~ /^(production)$/' #when sonar needs to be skiped for master branch
      #allow_failure: true

Validate Terraform:
  stage: Test and Lint
  script:
    - *set_aws_account_cred_script
    - cd deploy/
    - terraform init -backend=false
    - terraform validate
    - terraform fmt -check
    - echo "account_id=$account_id" >> ../build.env
  artifacts:
    reports:
      dotenv: build.env
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME =~ /^(hmnextwork)$/ || $CI_COMMIT_BRANCH =~ /^(hmnextwork)$/'

generate-cert:
  image: cr.siemens.com/blrise/platform/infrastructure/dockerhub/ise-maven-postgres:3.8.5-jdk-17-slim-pg-9.6
  stage: generate-cert
  script:
    - echo "This job runs once every 365 days"
    - cd deploy/
    - chmod +x openssl.sh
    - source ./openssl.sh
    - ls
  # only:
  #   variables:
  #     - $CI_PIPELINE_SOURCE == "schedule"
  artifacts:
    paths:
      - deploy/*.pem
    expire_in: never

Build:
  image: cr.siemens.com/blrise/platform/infrastructure/dockerhub/ise-node-angular:16.10.0-14.2.10-alpine-openjdk11
  stage: Build
  before_script: [""]
  script:
    - npm ci --cache .npm --prefer-offline
    - npm config set registry ${NPM_CONFIG_REGISTRY}
    - ng build --configuration=hmnextwork
  cache:
    paths:
      - .npm/
  artifacts:
    paths:
      - dist/
    expire_in: 1 hour
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /^(hmnextwork)$/'

Docker Image Push hmnextwork:
  stage: Docker Image Push
  variables:
    IMAGE_PATH: 858728894252.dkr.ecr.$AWS_REGION.amazonaws.com/nextwork-repo
    IMAGE_VERSION: ${CI_PROJECT_NAME}_${CI_PIPELINE_ID}_hmnextwork
  before_script: [""]
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  script:
    - echo "docker path = $ECR_IMAGE_PATH:$IMAGE_VERSION"
    - mkdir -p /root/.aws
    - export PROFILE="hmnextwork"
    - echo $CREDENTIALS | base64 -d > /root/.aws/credentials
    - echo "{\"credHelpers\":{\"$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\":\"ecr-login\"}}" > /kaniko/.docker/config.json
    - rm -rf target/*-sources.jar
    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $ECR_IMAGE_PATH:$IMAGE_VERSION --build-arg http_proxy=$http_proxy --build-arg https_proxy=$http_proxy --build-arg no_proxy=$no_proxy --build-arg APP_USER=$APP_USER --build-arg APP_DIR=$APP_DIR --build-arg PROFILE=$PROFILE
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /^(hmnextwork)$/'
  dependencies:
    - Validate Terraform
    - generate-cert
    #- build
  allow_failure: true
  artifacts:
    reports:
      dotenv: build.env


#Docker Image Push UAT: 
#  stage: Docker Image Push
#  variables:
#    IMAGE_PATH: 858728894252.dkr.ecr.$AWS_REGION.amazonaws.com/nextwork-repo
#    IMAGE_VERSION: ${CI_PROJECT_NAME}_${CI_PIPELINE_ID}_uat
#  before_script: [""]
#  image:
#    name: gcr.io/kaniko-project/executor:debug
#    entrypoint: [""]
#  script:
#    - echo "docker path = $ECR_IMAGE_PATH:$IMAGE_VERSION"
#    - mkdir -p /root/.aws
#    - export PROFILE="uat"
#    - echo $CREDENTIALS | base64 -d > /root/.aws/credentials
#    - echo "{\"credHelpers\":{\"$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\":\"ecr-login\"}}" > /kaniko/.docker/config.json
#    - rm -rf target/*-sources.jar
#    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $ECR_IMAGE_PATH:$IMAGE_VERSION --build-arg http_proxy=$http_proxy --build-arg https_proxy=$http_proxy --build-arg no_proxy=$no_proxy --build-arg APP_USER=$APP_USER --build-arg APP_DIR=$APP_DIR --build-arg PROFILE=$PROFILE
#  rules:
#    - if: '$CI_COMMIT_BRANCH == "uat"'
#  dependencies:
#    - Validate Terraform
#    #- build
#  allow_failure: true
#  artifacts:
#    reports:
#      dotenv: build.env
#
#Docker Image Push Production: 
#  stage: Docker Image Push
#  variables:
#    IMAGE_PATH: $account_id.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME-repo
#    IMAGE_VERSION: ${CI_PROJECT_NAME}_${CI_PIPELINE_ID}_production
#    AWS_ACCESS_KEY_ID: $PROD_AWS_ACCESS_KEY_ID
#    AWS_SECRET_ACCESS_KEY: $PROD_AWS_SECRET_ACCESS_KEY
#    AWS_REGION: $PROD_AWS_REGION
#  before_script: [""]
#  image:
#    name: gcr.io/kaniko-project/executor:debug
#    entrypoint: [""]
#  script:
#    - mkdir -p /root/.aws
#    - export PROFILE="production"
#    - echo $CREDENTIALS | base64 -d > /root/.aws/credentials
#    - echo "{\"credHelpers\":{\"$account_id.dkr.ecr.$AWS_REGION.amazonaws.com\":\"ecr-login\"}}" > /kaniko/.docker/config.json
#    - echo "docker path = $IMAGE_PATH:$IMAGE_VERSION"
#    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $IMAGE_PATH:$IMAGE_VERSION --build-arg http_proxy=$http_proxy --build-arg https_proxy=$http_proxy --build-arg no_proxy=$no_proxy --build-arg PROFILE=$PROFILE
#  rules:
#    - if: '$CI_COMMIT_BRANCH == "production"'

hmnextwork Plan:
  stage: hmnextwork Plan
  script:
    - *set_aws_account_cred_script
    - cd deploy/
    - *TF_VAR_backendconf_script
    - terraform init -upgrade -backend-config="s3_backend.config"
    - terraform workspace select hmnextwork || terraform workspace new hmnextwork
    - terraform plan -var-file=terraform_hmnextwork.tfvars
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /^(hmnextwork)$/'

hmnextwork Apply:
  stage: hmnextwork Apply
  script:
    - *set_aws_account_cred_script
    - cd deploy/
    - *TF_VAR_backendconf_script
    - terraform init -upgrade -backend-config="s3_backend.config"
    - terraform workspace select hmnextwork
    - terraform apply -auto-approve -var-file=terraform_hmnextwork.tfvars
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /^(hmnextwork)$/'

#UAT Plan:
#  stage: UAT Plan
#  script:
#    - *set_aws_account_cred_script
#    - cd deploy/
#    - *TF_VAR_backendconf_script
#    - terraform init -upgrade -backend-config="s3_backend.config"
#    - terraform workspace select uat || terraform workspace new uat
#    - terraform plan -var-file=terraform_uat.tfvars
#  rules:
#    - if: '$CI_COMMIT_BRANCH == "uat"'
#
#UAT Apply:
#  stage: UAT Apply
#  script:
#    - *set_aws_account_cred_script
#    - cd deploy/
#    - *TF_VAR_backendconf_script
#    - terraform init -upgrade -backend-config="s3_backend.config"
#    - terraform workspace select uat
#    - terraform apply -auto-approve -var-file=terraform_uat.tfvars
#  rules:
#    - if: '$CI_COMMIT_BRANCH == "uat"'
#
#Production Plan:
#  stage: Production Plan
#  script:
#    - cd deploy/
#    - *TF_VAR_backendconf_script
#    - terraform init -upgrade -backend-config="s3_backend.config"
#    - terraform workspace select production || terraform workspace new production
#    - terraform plan -var-file=terraform_production.tfvars
#  rules:
#    - if: '$CI_COMMIT_BRANCH == "production"'
#
#Production Apply:
#  stage: Production Apply
#  script:
#    - cd deploy/
#    - *TF_VAR_backendconf_script
#    - terraform init -upgrade -backend-config="s3_backend.config"
#    - terraform workspace select production
#    - terraform apply -auto-approve -var-file=terraform_production.tfvars
#  rules:
#    - if: '$CI_COMMIT_BRANCH == "production"'
#
hmnextwork Destroy:
  stage: Destroy
  script:
    - *set_aws_account_cred_script
    - cd deploy/
    - *TF_VAR_backendconf_script
    - terraform init -upgrade -backend-config="s3_backend.config"
    - terraform workspace select hmnextwork
    - terraform destroy -auto-approve -var-file=terraform_hmnextwork.tfvars
  needs: []
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /^(hmnextwork)$/'
      when: manual

#UAT Destroy:
#  stage: Destroy
#  script:
#    - *set_aws_account_cred_script
#    - cd deploy/
#    - *TF_VAR_backendconf_script
#    - terraform init -upgrade -backend-config="s3_backend.config"
#    - terraform workspace select uat
#    - terraform destroy -auto-approve -var-file=terraform_uat.tfvars
#  needs: []
#  rules:
#    - if: '$CI_COMMIT_BRANCH == "uat"'
#      when: manual
#
#Production Destroy:
#  stage: Destroy
#  script:
#    - cd deploy/
#    - *TF_VAR_backendconf_script
#    - terraform init -upgrade -backend-config="s3_backend.config"
#    - terraform workspace select production
#    - terraform destroy -auto-approve -var-file=terraform_production.tfvars
#  needs: []
#  rules:
#    - if: '$CI_COMMIT_BRANCH == "production"'
#      when: manual
#
###KTtest
